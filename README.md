# LLM-guided Federated Continual Learning (LLM-FCL)

Prototype research project exploring how **Large Language Models (LLMs)** can guide
**Federated Continual Learning (FCL)** strategies in image classification tasks.

## ğŸ“‚ Project Structure
llm_fcl_prototype
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ run_llm_fcl.py        # Main training loop (entry point)
â”‚   â”œâ”€â”€ data.py               # Dataset loading + client splits
â”‚   â”œâ”€â”€ model.py              # Model definitions (e.g., ResNet18)
â”‚   â”œâ”€â”€ fl.py                 # Federated Learning logic (Client, Server, FedAvg)
â”‚   â”œâ”€â”€ policy.py             # LLM-guided policy for tuning hyperparams
â”‚   â””â”€â”€ strategies            # Continual learning strategies
â”‚       â”œâ”€â”€ replay.py         # Replay buffer
â”‚       â””â”€â”€ ewc.py            # Elastic Weight Consolidation (optional)
â”œâ”€â”€ prompts
â”‚   â””â”€â”€ policy_prompt.txt     # Prompt template for the LLM policy
â”œâ”€â”€ experiments
â”‚   â””â”€â”€ plan.md               # Experiment plan & notes
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # Documentation
â””â”€â”€ .gitignore                # Git ignore file

---

## ğŸš€ Getting Started

### 1. Clone the repo
```bash
git clone https://github.com/<your-username>/llm_fcl_prototype.git
cd llm_fcl_prototype

2. Install dependencies
pip install -r requirements.txt

3. Run the prototype (step-by-step implementation)
python -m src.run_llm_fcl

ğŸ“ Roadmap
	â€¢	Repo scaffold created
	â€¢	Implement data loading (CIFAR-100 with non-IID client splits)
	â€¢	Add ResNet18 model
	â€¢	Implement Federated Learning (FedAvg)
	â€¢	Add Continual Learning strategies (Replay, EWC)
	â€¢	Connect LLM-guided policy
	â€¢	Run experiments & generate plots

   ğŸ“– Research Context

This project builds on previous work in:
	â€¢	Federated Learning (FL): collaborative model training without centralizing data.
	â€¢	Continual Learning (CL): adapting models to evolving data streams while mitigating catastrophic forgetting.
	â€¢	Federated Continual Learning (FCL): combines FL and CL, but suffers from instability under non-IID data.
	â€¢	LLMs for meta-learning: here we explore if LLMs can guide hyperparameter tuning or replay strategies dynamically.

â¸»

ğŸ“Š Planned Experiments
	â€¢	Baselines: FedAvg + Replay (fixed), FedAvg + Replay + EWC (fixed).
	â€¢	LLM-FCL: LLM-guided dynamic tuning of replay ratio, learning rate, and EWC Î».
	â€¢	Datasets: CIFAR-100, TrashNet, and optionally DWRL if available.
	â€¢	Metrics: accuracy, per-class recall, forgetting, stability.

â¸»

ğŸ§‘â€ğŸ’» Authors
	â€¢	Somayeh Shami (PhD candidate, TU Graz)
	â€¢	Collaborators: [to be added]
## ğŸ“Š Results (CIFAR-100 pilot)

**Setup:** ResNet-18 (ImageNet pretrained), 4 clients (Î±=0.2), 2 epochs/round, 3k images/client, batch 128, SGD lr=1e-3, replay on.

| Round | Baseline Acc | Policy Acc | Policy LR | Policy Replay |
|------:|-------------:|-----------:|-----------:|--------------:|
| -1    | 0.010        | 0.010      | 0.00800    | 0.20 |
| 0     | 0.052        | 0.129      | 0.00960    | 0.25 |
| 1     | 0.164        | 0.404      | 0.01152    | 0.30 |
| 2     | 0.274        | 0.561      | 0.01382    | 0.35 |
| 3     | 0.335        | 0.621      | 0.01659    | 0.40 |
| 4     | 0.378        | 0.658      | â€”          | â€” |

**Summary:** The adaptive policy outperforms fixed hyperparameters by **+28.0 pp** at Round 4 (65.8% vs 37.8%), mainly by ramping LR (â‰ˆ0.008â†’0.0166) and replay (0.20â†’0.40).  
See details in `experiments/results/cifar100_policy_vs_baseline_subset3000_rounds5.md`.

